{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Word2Vec_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxRfgo_b9A_j"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5A3wxek9C1y",
        "outputId": "daac9604-3587-4356-82f4-21066587611a"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "nltk.download('brown')\r\n",
        "from nltk.corpus import brown\r\n",
        "from sklearn.metrics.pairwise import pairwise_distances\r\n",
        "\r\n",
        "import random\r\n",
        "from datetime import datetime\r\n",
        "import math\r\n",
        "import json\r\n",
        "import glob"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctjBroSX9HUb"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL0abQi_9Gsb"
      },
      "source": [
        "def build_vocabulary(corpus, n_vocab):\r\n",
        "\r\n",
        "  idx_sents = []\r\n",
        "  idx_count = 1\r\n",
        "\r\n",
        "\r\n",
        "  word2idx = {'<UNK>': 0}\r\n",
        "  word_counts = {}\r\n",
        "\r\n",
        "  sentences = []\r\n",
        "\r\n",
        "  for sent in corpus:\r\n",
        "    for tok in sent:\r\n",
        "      tok = tok.lower()\r\n",
        "      if tok not in word_counts:\r\n",
        "        word_counts[tok] = 1\r\n",
        "\r\n",
        "      else: word_counts[tok] += 1\r\n",
        "\r\n",
        "  n_vocab = min(n_vocab, len(word_counts))\r\n",
        "  word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\r\n",
        "  top_words = [w for w, count in word_counts[:n_vocab-1]] + [0]\r\n",
        "\r\n",
        "  for sent in corpus:\r\n",
        "    for tok in sent:\r\n",
        "      if tok in top_words:\r\n",
        "        if tok not in word2idx:\r\n",
        "          word2idx[tok] = idx_count\r\n",
        "          idx_count += 1\r\n",
        "\r\n",
        "    sentence = [word2idx[token] if token in word2idx else 0 for token in sent]\r\n",
        "    sentences.append(sentence)\r\n",
        "\r\n",
        "  return sentences, word2idx\r\n",
        "\r\n",
        "\r\n",
        "def find_analogies( w1, w2, w3, concat=True, we_file ='w2v_model.npz', w2i_file='w2v_word2idx.json'):\r\n",
        "  npz = np.load(we_file)\r\n",
        "  W1 = npz['arr_0']\r\n",
        "  W2 = npz['arr_1']\r\n",
        "\r\n",
        "  with open(w2i_file) as f:\r\n",
        "    word2idx = json.load(f)\r\n",
        "  \r\n",
        "  idx2word = {y:x for x,y in word2idx.items()}\r\n",
        "\r\n",
        "  vocab_sz = len(word2idx)\r\n",
        "\r\n",
        "  if concat:\r\n",
        "    We = np.hstack([W1, W2.T])\r\n",
        "\r\n",
        "    print(\"We shape: \", We.shape)\r\n",
        "    assert(vocab_sz == We.shape[0])\r\n",
        "\r\n",
        "  else:\r\n",
        "    We = (W1 + W2.T)/2\r\n",
        "  \r\n",
        "  V, D = We.shape\r\n",
        "\r\n",
        "  king = We[word2idx[w1]]\r\n",
        "  man = We[word2idx[w2]]\r\n",
        "  woman = We[word2idx[w3]]\r\n",
        "  v0 = king - man + woman\r\n",
        "\r\n",
        " \r\n",
        "  distances = pairwise_distances(v0.reshape(1, D), We, metric=dist).reshape(V)\r\n",
        "  idx = distances.argsort()[:4]\r\n",
        "  best_idx = -1\r\n",
        "  keep_out = [word2idx[w] for w in (w1, w2, w3)]\r\n",
        "  for i in idx:\r\n",
        "      if i not in keep_out:\r\n",
        "          best_idx = i\r\n",
        "          break\r\n",
        "  best_word = idx2word[best_idx]\r\n",
        "\r\n",
        "  print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)\r\n",
        "\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "  return 1 / (1 + np.exp(-x))\r\n",
        "\r\n",
        "def plot_cost(self, cost, title):\r\n",
        "  plt.plot(cost)\r\n",
        "  plt.title(title)\r\n",
        "  plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd7f2ynS9Dfz"
      },
      "source": [
        "# Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKNPESwIAG3i"
      },
      "source": [
        "class Word2Vec():\r\n",
        "\r\n",
        "  def __init__(self, embedding_sz, vocab_sz, context_window = 3):\r\n",
        "    self.embedding_sz = embedding_sz\r\n",
        "    self.vocab_sz = vocab_sz\r\n",
        "    self.context_window = context_window\r\n",
        "     \r\n",
        "    # The Embedding Layer (center word)\r\n",
        "    self.W1 = np.random.rand(vocab_sz, embedding_sz)\r\n",
        "\r\n",
        "    # The Dense Layer (context words)\r\n",
        "    self.W2 = np.random.rand(embedding_sz, vocab_sz)\r\n",
        "\r\n",
        "  \r\n",
        "  def _get_neg_prob(self, corpus):\r\n",
        "    self.neg_prob = np.zeros(self.vocab_sz)\r\n",
        "    word_counts = {}\r\n",
        "    word_sum = sum(len(sent) for sent in corpus)\r\n",
        "\r\n",
        "    for sent in corpus:\r\n",
        "      for word in sent:\r\n",
        "        if word not in word_counts:\r\n",
        "          word_counts[word] = 1\r\n",
        "        \r\n",
        "        else: \r\n",
        "          word_counts[word] += 1\r\n",
        "    \r\n",
        "    for i in range(self.vocab_sz):\r\n",
        "      # 0.75 is a hyperparameter that was found to work well after hyperparameter tunining. \r\n",
        "      # This reduces the probability of picking extemely frequent words relative to less frequent ones\r\n",
        "      self.neg_prob[i] = (word_counts[i] / word_sum)**0.75\r\n",
        "    \r\n",
        "    # asserting that we have as many word counts as the size of neg_prob\r\n",
        "    assert(all(self.neg_prob >0))\r\n",
        "    \r\n",
        "    return self.neg_prob\r\n",
        "\r\n",
        "  \r\n",
        "  def _get_neg_samples(self, context, num_neg_samples):\r\n",
        "    \r\n",
        "    temp = {}\r\n",
        "    for context_idx in context:\r\n",
        "      temp[context_idx] = self.neg_prob[context_idx]\r\n",
        "      self.neg_prob[context_idx] = 0\r\n",
        "\r\n",
        "    neg_samples = np.random.choice(\r\n",
        "        self.vocab_sz,\r\n",
        "        size = num_neg_samples,\r\n",
        "        replace = False,\r\n",
        "        p = self.neg_prob/np.sum(self.neg_prob)\r\n",
        "    )\r\n",
        "\r\n",
        "    for context_idx in temp:\r\n",
        "      self.neg_prob[context_idx] = temp[context_idx]\r\n",
        "    \r\n",
        "    return neg_samples\r\n",
        "  \r\n",
        "  def _get_prob_vector(self,center_word, context,isNeg = False):\r\n",
        "    # multiplying the context and center_word\r\n",
        "    A = center_word.dot(self.W2[:,context])\r\n",
        "    if isNeg == False:\r\n",
        "      # vect of dim (context,) => used for the objective function that is maximized when the probability is closest to 1\r\n",
        "      prob_vector = sigmoid(A)\r\n",
        "    else: \r\n",
        "      # vect of dim (context,) => used for the objective function that is maximized when the probability is closest to 0\r\n",
        "      prob_vector = sigmoid(-A)\r\n",
        "\r\n",
        "    return prob_vector\r\n",
        "  \r\n",
        "  def backprop(self, p, q, center_word, center_word_idx, context, neg_samples, dW1, dW2, lr):\r\n",
        "\r\n",
        "    # no need to update the whole ebedding matrix as we are using negative sampling. just updating the context and negative samples\r\n",
        "  \r\n",
        "    dW2[:, context] = np.outer(center_word, p - 1)\r\n",
        "    # 1-q = center_word.dot(self.W2[:,neg_samples])\r\n",
        "    dW2[:, neg_samples] = np.outer(center_word, 1-q)\r\n",
        "\r\n",
        "    self.W2[:, context] -= lr*dW2[:, context]\r\n",
        "    self.W2[:, neg_samples] -= lr*dW2[:, neg_samples]\r\n",
        "\r\n",
        "    dW1 = (p - 1).dot(self.W2[:,context].T) + (1 - q).dot(self.W2[:,neg_samples].T)\r\n",
        "          \r\n",
        "    self.W1[center_word_idx,:] -= lr*dW1\r\n",
        "  \r\n",
        "\r\n",
        "  def fit(self, corpus, num_neg_samples = 10, lr = 3e-5, epochs = 10):\r\n",
        "    num_samples = len(corpus)\r\n",
        "    vocab_sz = self.vocab_sz\r\n",
        "    embedding_sz = self.embedding_sz\r\n",
        "    self._get_neg_prob(corpus)\r\n",
        "\r\n",
        "    cost_per_epoch = []\r\n",
        "    samples_idx = list(range(num_samples))\r\n",
        "\r\n",
        "    dW1 = np.zeros(self.W1.shape)\r\n",
        "    dW2 = np.zeros(self.W2.shape)\r\n",
        "\r\n",
        "    for i in range(epochs):\r\n",
        "      t0 = datetime.now()\r\n",
        "      random.shuffle(samples_idx)\r\n",
        "      costs_per_epoch = []\r\n",
        "      glob_count = 0\r\n",
        "\r\n",
        "      for sent_idx in range(num_samples):\r\n",
        "        glob_count += 1\r\n",
        "\r\n",
        "        if glob_count % 5000 == 0:\r\n",
        "          print(glob_count, \" sentences done\")\r\n",
        "\r\n",
        "        j = samples_idx[sent_idx]\r\n",
        "        sent = corpus[sent_idx]\r\n",
        "\r\n",
        "        if len(sent) < 2 * self.context_window + 1:\r\n",
        "          continue\r\n",
        "      \r\n",
        "        costs_per_sent = []\r\n",
        "        num_words = len(sent)\r\n",
        "\r\n",
        "        for w_idx in range(num_words):\r\n",
        "\r\n",
        "          # center word : X*T.W1 where X is a one hot encoded vector with a 1 at the vocabulary index\r\n",
        "          center_word = self.W1[sent[w_idx], :]\r\n",
        "\r\n",
        "          start = max(0, w_idx - self.context_window)\r\n",
        "          end = min(num_words, w_idx + 1 + self.context_window)\r\n",
        "\r\n",
        "          context = np.concatenate((sent[start:w_idx], sent[w_idx + 1:end]))\r\n",
        "          context = np.array(list(set(context)), dtype = np.int32)\r\n",
        "\r\n",
        "          # getting the negative samples\r\n",
        "          neg_samples = self._get_neg_samples(context, num_neg_samples)\r\n",
        "\r\n",
        "          p = self._get_prob_vector(center_word,context, isNeg = False)\r\n",
        "          q = self._get_prob_vector(center_word,neg_samples, isNeg = True)\r\n",
        "          # cost function : negative log loss\r\n",
        "\r\n",
        "          cost = - (np.log(q).sum() + np.log(p).sum())\r\n",
        "          costs_per_sent.append(cost/(num_neg_samples + len(context)))\r\n",
        "\r\n",
        "          self.backprop(p, q, center_word, sent[w_idx], context, neg_samples, dW1, dW2, lr)\r\n",
        "\r\n",
        "        costs_per_epoch.append(np.mean(costs_per_sent))\r\n",
        "\r\n",
        "      cost_per_epoch.append(np.mean(costs_per_epoch))\r\n",
        "      print ( \"time to complete epoch %d:\" % i,  (datetime.now()-t0), \"cost:\", cost_per_epoch[-1])\r\n",
        "\r\n",
        "      plot_cost(costs_per_epoch,'Numpy costs')\r\n",
        "\r\n",
        "    plot_cost(cost_per_epoch, 'Numpy cost at each epoch')\r\n",
        "\r\n",
        "\r\n",
        "  def save(self, fn):\r\n",
        "    arrays = [self.W1, self.W2]\r\n",
        "    np.savez(fn, *arrays)\r\n",
        "\r\n",
        "\r\n",
        "def run():\r\n",
        "  sentences, word2idx = build_vocabulary(brown.sents(), n_vocab = 6000)\r\n",
        "  with open('w2v_word2idx.json', 'w') as f:\r\n",
        "    json.dump(word2idx, f)\r\n",
        "\r\n",
        "  vocab_sz = len(word2idx)\r\n",
        "  model = Word2Vec(80, vocab_sz, 3)\r\n",
        "  model.fit(sentences, lr = 1e-4, epochs = 20)\r\n",
        "  model.save('w2v_model.npz')\r\n",
        "\r\n",
        "if __name__=='__main__':\r\n",
        "  run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
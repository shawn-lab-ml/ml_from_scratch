{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_from_scratch_AIAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYZtq-iOwLxM",
        "colab_type": "text"
      },
      "source": [
        "# Recreating \"Attention is All you Need\"'s Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8YWus_3SYc",
        "colab_type": "text"
      },
      "source": [
        "\"Attention is All you Need\" (by Ashish Vaswan, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin) is one of the most important papers in NLP.\n",
        "\n",
        "The previous methods included RNNs which were both inherently diffcult to train properly as they operate sequentially using recurrence. Therefore parallelization was not possible.\n",
        "\n",
        "\"the Transformer used no recurrence, instead processing all words or symbols in the sequence in parallel while making use of a self-attention mechanism to incorporate context from words farther away.\" Therefore the Transformer fixed the issue of parallelization as well as that of long sequence. Moreover, Transformers produced some notable BLEU score improvements in seq2seq tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXekFFXl69Ue",
        "colab_type": "text"
      },
      "source": [
        "The main sources used for the recreation of the Transformer used in \"Attention is All you Need\" are:\n",
        "\n",
        "\n",
        "*   The Paper itself: https://arxiv.org/pdf/1706.03762v5.pdf\n",
        "*   Samuel Lynn-Evans: https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n",
        "*   Aladdin Pearson: https://www.youtube.com/watch?v=U0s0f995w14\n",
        "*   Pytorch Tutorial: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEeWlcJDwj_E",
        "colab_type": "text"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNkkiODmwMbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p7VTHdw-bTr",
        "colab_type": "text"
      },
      "source": [
        "# **config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDE2Pv-5-doB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are the configs chosen in the AIAN paper\n",
        "D_MODEL = 512\n",
        "D_FF = 2048\n",
        "MAX_LEN = 5000\n",
        "DROPOUT = 0.1\n",
        "NUM_LAYERS = 6\n",
        "N_HEADS = 8"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnqm3qbWw0BH",
        "colab_type": "text"
      },
      "source": [
        "# **Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdtegRsUUwMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As we are not using RNNs, we need not only know what is the meaning of the word (word embedding) but also what is its position (position encoding)\n",
        "# PE_(pos, 2i) = sin(pos/10000^(2i/d_model) where pos is the position and i is the dimension\n",
        "# PE_(pos, 2i+1) = cos(pos/10000^(2i/d_model)) where pos is the position and i is the dimension\n",
        "# Positional Encoding can be replaced by learned positional Embedding (nn.Embedding(max_len, encoding_size))\n",
        "\n",
        "# After attempting different methods for Positional Encoding I found this one on Pytorch (link above), fastest option by far\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout, max_len):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pe[:x.size(0), :]\n",
        "        return x"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hayEuXm1GfvE",
        "colab_type": "text"
      },
      "source": [
        "# **Transformer Block**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dxGhcUNDBX0",
        "colab_type": "text"
      },
      "source": [
        "## *Multi-Head Self Attention*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Tq0EqxwMgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    # represents dk in the paper\n",
        "    self.head_dim = d_model // n_heads\n",
        "\n",
        "    assert(self.head_dim * n_heads == d_model), \"d_model must be dvisible by heads\"\n",
        "\n",
        "\n",
        "    self.q_linear = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.k_linear = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.v_linear = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "\n",
        "    # equivalent to saying nn.fc(d_model, d_model) however it is clearer this way\n",
        "    # as the output of the SelfAttention has shape (N, query_len,self.head_dim*n_heads)\n",
        "    self.fc_out = nn.Linear(self.head_dim*n_heads, d_model)\n",
        "\n",
        "  def forward(self, key, query, value, mask):\n",
        "    # N is your batch size\n",
        "    N = query.shape[0]\n",
        "    \n",
        "    # length of the vectors\n",
        "    value_len, query_len, key_len = value.shape[1], query.shape[1], key.shape[1]\n",
        "    \n",
        "    # reshaping the value, key and query from (N, value_len, d_model) to (N, value_len, n_heads, head_dim)\n",
        "    value = value.reshape(N, value_len, self.n_heads, self.head_dim)\n",
        "    query = query.reshape(N, query_len, self.n_heads, self.head_dim)\n",
        "    key = key.reshape(N, key_len, self.n_heads, self.head_dim)\n",
        "\n",
        "    x = self.selfAttention(key, query, value, mask)\n",
        "    x = self.fc_out(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  def selfAttention(self, key, query, value, mask):\n",
        "    N = query.shape[0]\n",
        "    query_len = query.shape[1]\n",
        "\n",
        "    query = self.q_linear(query)\n",
        "    key = self.k_linear(key)\n",
        "    value = self.v_linear(value)\n",
        "\n",
        "    qk = torch.einsum('nqhd, nkhd -> nhqk', [query, key])\n",
        "    x = qk/(self.head_dim**(1/2))\n",
        "\n",
        "    # optional mask which allows us to zero the attention outputs wherever there is padding in the input sentences\n",
        "    if mask is not None:\n",
        "      x = x.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "    \n",
        "    x = F.softmax(x, dim = 3)\n",
        "\n",
        "    # query_len = key_len = value_len\n",
        "    x = torch.einsum(\"nhqk, nkhd-> nqhd\", [x, value]).reshape (N, query_len, self.n_heads*self.head_dim)\n",
        "    return x"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1TDH-s0GTRn",
        "colab_type": "text"
      },
      "source": [
        "## *Feed Forward*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S9f8KZsiipF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
        "# connected feed-forward network, which is applied to each position separately and identically. This\n",
        "# consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  #The dimensionality of input and output is dmodel, and the inner-layer has dimensionality d_ff\n",
        "  def __init__(self, d_model, dropout, d_ff):\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.dropout(F.relu(self.linear_1(x)))\n",
        "    x = self.linear_2(x)\n",
        "    return x"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FteBdVtYGcjp",
        "colab_type": "text"
      },
      "source": [
        "## *Transformer Block*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyc6otVOBSkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the transformer block MHA -> Norm -> FF -> Norm\n",
        "# It represents the EncoderBlock and part of the DecoderBlock\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, dropout, d_ff):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mha = MultiHeadSelfAttention(d_model, n_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.feedForward = FeedForward(d_model, dropout, d_ff)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, key, query, value, mask):\n",
        "    x = self.mha(key, query, value, mask)\n",
        "    x = self.dropout(self.norm1(x + query))\n",
        "    ff = self.feedForward(x)\n",
        "    x = self.dropout(self.norm2(ff + x))\n",
        "    return x"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr4B24VrG7_D",
        "colab_type": "text"
      },
      "source": [
        "# **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot5I6TB3nm_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, src_vocab_size, d_model, n_heads, dropout, d_ff, num_layers, max_len):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "    self.positionalEncoding = PositionalEncoding(d_model, dropout=dropout, max_len=max_len)\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "    self.layers = nn.ModuleList(\n",
        "            [\n",
        "             TransformerBlock(\n",
        "                 d_model, \n",
        "                 n_heads, \n",
        "                 dropout, \n",
        "                 d_ff\n",
        "                 )\n",
        "             for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "  \n",
        "  def forward(self, x, mask):\n",
        "    x1 = self.embedding(x)\n",
        "    x2 = self.positionalEncoding(x)\n",
        "    x = self.dropout(x1 + x2)\n",
        "\n",
        "  # each x,x,x = key, query and value\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, x, x, mask)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUB87dsmJPh2",
        "colab_type": "text"
      },
      "source": [
        "# **Decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjsCfkG-JSYa",
        "colab_type": "text"
      },
      "source": [
        "## *Decoder Block*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUU9HT7Yu5Mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, dropout, d_ff):\n",
        "    super().__init__()\n",
        "    self.mha = MultiHeadSelfAttention(d_model, n_heads)\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "    self.transformerBlock = TransformerBlock(\n",
        "            d_model, \n",
        "            n_heads, \n",
        "            dropout, \n",
        "            d_ff\n",
        "        )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, enc_value, enc_key, src_mask, trg_mask):\n",
        "    attention = self.mha(x, x, x, trg_mask)\n",
        "    dec_query = self.dropout(self.norm(attention + x))\n",
        "    x = self.transformerBlock(enc_key, dec_query, enc_value, src_mask)\n",
        "    return x"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M77H-PBJyyK",
        "colab_type": "text"
      },
      "source": [
        "## *Decoder*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5q9Vnl1sgHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, trg_vocab_size, d_model, n_heads, dropout, d_ff, num_layers, max_len):\n",
        "    super(Decoder, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.max_len = max_len\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "    self.positionalEncoding = PositionalEncoding(d_model, dropout = dropout, max_len=max_len)\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "    self.layers = nn.ModuleList(\n",
        "            [\n",
        "             DecoderBlock(\n",
        "                 d_model, \n",
        "                 n_heads, \n",
        "                 dropout, \n",
        "                 d_ff\n",
        "                 )\n",
        "             for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "    self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "    \n",
        "  def forward(self, x, x_enc, src_mask, trg_mask):\n",
        "    x1 = self.embedding(x)\n",
        "    x2 = self.positionalEncoding(x)\n",
        "    x = self.dropout(x1 + x2)\n",
        "\n",
        "  \n",
        "    for layer in self.layers:\n",
        "      x = layer(x, x_enc, x_enc, src_mask, trg_mask)\n",
        "    \n",
        "    x = self.fc_out(x)\n",
        "    return x"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaxcQtPDKScZ",
        "colab_type": "text"
      },
      "source": [
        "# **Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpWAsMfTx7Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, d_model=512, num_layers=6, d_ff=2048, n_heads=8, dropout = 0.1, device=\"cuda\", max_len=1000):\n",
        "      super(Transformer, self).__init__()\n",
        "\n",
        "      self.encoder = Encoder(src_vocab_size, \n",
        "                             d_model, \n",
        "                             n_heads, \n",
        "                             dropout, \n",
        "                             d_ff, \n",
        "                             num_layers, \n",
        "                             max_len)\n",
        "      \n",
        "      self.decoder = Decoder(trg_vocab_size, \n",
        "                             d_model, \n",
        "                             n_heads, \n",
        "                             dropout, \n",
        "                             d_ff, \n",
        "                             num_layers, \n",
        "                             max_len)\n",
        "      \n",
        "      self.src_pad_idx = src_pad_idx\n",
        "      self.trg_pad_idx = trg_pad_idx\n",
        "      self.device = device\n",
        "  \n",
        "  def forward(self, x_src, x_trg):\n",
        "      src_mask = self.makeSrcMask(x_src)\n",
        "      trg_mask = self.makeTrgMask(x_trg)\n",
        "\n",
        "      x_enc = self.encoder(x_src, src_mask) \n",
        "      x_dec = self.decoder(x_trg, x_enc, src_mask, trg_mask)\n",
        "      return x_dec\n",
        "  \n",
        "  def makeSrcMask(self, x_src):\n",
        "      src_mask = (x_src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "      # (N, 1, 1, src_len)\n",
        "      return src_mask.to(self.device)\n",
        "      \n",
        "  def makeTrgMask(self, x_trg):\n",
        "      N, x_trg_len = x_trg.shape\n",
        "      trg_mask = torch.tril(torch.ones((x_trg_len, x_trg_len))).expand(\n",
        "          N, 1, x_trg_len, x_trg_len)\n",
        "        \n",
        "      return trg_mask.to(self.device)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzCNje1BfLUZ",
        "colab_type": "text"
      },
      "source": [
        "# **Tesing If Code Works**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6sj9yoCLU4D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "51029dba-f8c1-4492-e0dc-5c3088327c3e"
      },
      "source": [
        "import time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    src_pad_idx = 0\n",
        "    trg_pad_idx = 0\n",
        "    src_vocab_size = 10\n",
        "    trg_vocab_size = 10\n",
        "\n",
        "    x = torch.tensor([[6,8,5,7,3,1,2,6,0], [3,4,5,2,1,5,6,7,0]]).to(device)\n",
        "    trg = torch.tensor([[3,4,5,2,9,4,3,2,6], [2,5,6,9,1,3,0,0,0]]).to(device)\n",
        "\n",
        "    model = Transformer(src_vocab_size=src_vocab_size, trg_vocab_size=trg_vocab_size, src_pad_idx=src_pad_idx, trg_pad_idx=trg_pad_idx).to(device)\n",
        "    output = model(x, trg[:, :-1])\n",
        "    print(output.shape)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 10])\n",
            "--- 0.3390934467315674 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck8pzNQbLyKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
